(window.webpackJsonp=window.webpackJsonp||[]).push([[46],{441:function(t,a,s){t.exports=s.p+"assets/img/packagist.835e6f2c.png"},570:function(t,a,s){"use strict";s.r(a);var e=s(29),n=Object(e.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"dm4t-pilot"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#dm4t-pilot"}},[t._v("#")]),t._v(" DM4T Pilot")]),t._v(" "),e("h2",{attrs:{id:"pilot-name"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#pilot-name"}},[t._v("#")]),t._v(" Pilot Name")]),t._v(" "),e("p",[t._v("Data Management for TEDDINET (DM4T)")]),t._v(" "),e("h2",{attrs:{id:"authors"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#authors"}},[t._v("#")]),t._v(" Authors")]),t._v(" "),e("p",[t._v("Julian Padget (DM4T), Dan Fowler (OKI), Evgeny Karev (OKI)")]),t._v(" "),e("h2",{attrs:{id:"field"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#field"}},[t._v("#")]),t._v(" Field")]),t._v(" "),e("p",[t._v("Energy Data")]),t._v(" "),e("h2",{attrs:{id:"fd-tech-involved"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#fd-tech-involved"}},[t._v("#")]),t._v(" FD Tech Involved")]),t._v(" "),e("ul",[e("li",[t._v("Frictionless Data specs: "),e("a",{attrs:{href:"http://specs.frictionlessdata.io/",target:"_blank",rel:"noopener noreferrer"}},[t._v("http://specs.frictionlessdata.io/"),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("Data Package Pipelines: "),e("a",{attrs:{href:"https://github.com/frictionlessdata/datapackage-pipelines",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/frictionlessdata/datapackage-pipelines"),e("OutboundLink")],1)]),t._v(" "),e("li",[t._v("Goodtables: "),e("a",{attrs:{href:"https://github.com/frictionlessdata/goodtables-py",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/frictionlessdata/goodtables-py"),e("OutboundLink")],1)])]),t._v(" "),e("p",[e("code",[t._v("packagist")]),t._v(" has now moved to "),e("a",{attrs:{href:"http://create.frictionlessdata.io",target:"_blank",rel:"noopener noreferrer"}},[t._v("create.frictionlessdata.io"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"context"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#context"}},[t._v("#")]),t._v(" Context")]),t._v(" "),e("h3",{attrs:{id:"problem-we-were-trying-to-solve"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#problem-we-were-trying-to-solve"}},[t._v("#")]),t._v(" Problem We Were Trying To Solve")]),t._v(" "),e("p",[t._v("Open Knowledge International and the Data Management for TEDDINET project (DM4T) agreed to work together on a proof-of-concept pilot to attempt to use Frictionless Data approaches to address some of the data legacy issues facing the TEDDINET project, a research network addressing the challenges of transforming energy demand in our buildings, as a key component of the transition to an affordable, low carbon energy system. The problem as described on the DM4T Website:")]),t._v(" "),e("blockquote",[e("p",[t._v("The Engineering and Physical Sciences Research Council (EPSRC), the UK’s main agency for funding research in engineering and the physical sciences, funded 22 projects over two calls in 2010 and 2012 to investigate Transforming Energy Demand through Digital Innovation’ (TEDDI) as a means to find out how people use energy in homes and what can be done reduce energy consumption. A lot of data is being collected at different levels of detail in a variety of housing throughout the UK, but the level of detail are largely defined by the needs of each individual project. At the same time, the Research Councils UK (RCUK) are defining guidelines for what happens to data generated by projects they fund which require researchers to take concrete actions to store, preserve, and document their data for future reference.")])]),t._v(" "),e("blockquote",[e("p",[t._v("The problem, however, is that there is relatively little awareness, limited experience and only emerging practice of how to incorporate data management into much of physical science research. This is in contrast to established procedures for data formats and sharing in the biosciences, stemming from international collaboration on the Human Genome Project, and in the social sciences, where data from national surveys, including census data, have been centrally archived for many years. Consequently, current solutions may be able to meet a minimal interpretation of the requirements, but not effectively deliver the desired data legacy.")])]),t._v(" "),e("p",[t._v("The DM4T group selected three suitable datasets to on which to base this work and provided domain knowledge to ensure the pilot is applicable to real use cases.")]),t._v(" "),e("p",[t._v("Output was tracked here: "),e("a",{attrs:{href:"https://github.com/frictionlessdata/pilot-dm4t/issues",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/frictionlessdata/pilot-dm4t/issues"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"the-work"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#the-work"}},[t._v("#")]),t._v(" The work")]),t._v(" "),e("p",[t._v("We will use the "),e("code",[t._v("refit-cleaned")]),t._v(" dataset to show the Frictionless Data specs and software capabilities. For this work, we limited the size of this dataset in order to preserve a reasonable showcasing time. However, by design the Frictionless Data software has a very good scalability and this process could be reproduced for the whole dataset. But for now it is worth noting that the speed for such a big datasets could be a bottle neck for a research work.")]),t._v(" "),e("h3",{attrs:{id:"refit-electrical-load-measurements-cleaned"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#refit-electrical-load-measurements-cleaned"}},[t._v("#")]),t._v(" REFIT: Electrical Load Measurements (Cleaned)")]),t._v(" "),e("blockquote",[e("p",[t._v("Link to the dataset: "),e("a",{attrs:{href:"https://github.com/frictionlessdata/pilot-dm4t/tree/delivery/datasets/refit-cleaned",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/frictionlessdata/pilot-dm4t/tree/delivery/datasets/refit-cleaned"),e("OutboundLink")],1)])]),t._v(" "),e("p",[t._v("For each house in the study, this dataset consists of granular readings of electrical load.  There were 20 houses in total, and each house had a different mix of devices plugged into the electrical load sensor.  The dataset was distributed as a zipped file (~500MB) containing 20 CSVs with a combined ~120 million rows.")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("Time,Unix,Aggregate,Appliance1,Appliance2,Appliance3,Appliance4,Appliance5,Appliance6,Appliance7,Appliance8,Appliance9\n2013-10-09 13:06:17,1381323977,523,74,0,69,0,0,0,0,0,1\n2013-10-09 13:06:31,1381323991,526,75,0,69,0,0,0,0,0,1\n2013-10-09 13:06:46,1381324006,540,74,0,68,0,0,0,0,0,1\n2013-10-09 13:07:01,1381324021,532,74,0,68,0,0,0,0,0,1\n2013-10-09 13:07:15,1381324035,540,74,0,69,0,0,0,0,0,1\n2013-10-09 13:07:18,1381324038,539,74,0,69,0,0,0,0,0,1\n2013-10-09 13:07:30,1381324050,537,74,0,69,0,0,0,0,0,1\n2013-10-09 13:07:32,1381324052,537,74,0,69,0,0,0,0,0,1\n2013-10-09 13:07:44,1381324064,548,74,0,69,0,0,0,0,0,1\n")])])]),e("p",[t._v("Given that these datasets were already provided in well structured CSV files, it was straightforward to translate the data dictionary found in the dataset’s README into the relevant fields in the datapackage.json.  We did not need to alter the CSVs that comprise the dataset.")]),t._v(" "),e("h3",{attrs:{id:"creating-a-data-package-using-datapackage-pipelines"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#creating-a-data-package-using-datapackage-pipelines"}},[t._v("#")]),t._v(" Creating a data package using Datapackage Pipelines")]),t._v(" "),e("blockquote",[e("p",[t._v("Link to the Datapackage Pipelines project: "),e("a",{attrs:{href:"https://github.com/frictionlessdata/datapackage-pipelines",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/frictionlessdata/datapackage-pipelines"),e("OutboundLink")],1)])]),t._v(" "),e("p",[t._v("Datapackage Pipelines is a framework for declarative stream-processing of tabular data. It is built upon the concepts and tooling of the Frictionless Data project. The basic concept in this framework is the pipeline. A pipeline has a list of processing steps, and it generates a single data package as its output. Pipelines are defined in a declarative way, not in code. One or more pipelines can be defined in a "),e("code",[t._v("pipeline-spec.yaml")]),t._v(" file. This file specifies the list of processors (referenced by name) and their execution parameters.")]),t._v(" "),e("p",[t._v("One of the main purposes of the Frictionless Data project is data containerization. It means that instead of having two separated data knowledge sources (data files and text readme), we’re going to put both of them into a container based on the "),e("code",[t._v("Data Package")]),t._v(" specification. This allows us to:")]),t._v(" "),e("ul",[e("li",[t._v("Ensure that the dataset description is shipped with the data files")]),t._v(" "),e("li",[t._v("Provide column data type information to allow type validation")]),t._v(" "),e("li",[t._v("Use the Frictionless Data tooling for reading and validating datasets")]),t._v(" "),e("li",[t._v("Enable usage of other software which supports Frictionless Data specifications")])]),t._v(" "),e("p",[t._v("First, we used the "),e("code",[t._v("datapackage-pipeline")]),t._v(" library to create a data package from the raw dataset. We need a declarative file called "),e("code",[t._v("datapackage-pipelines.yaml")]),t._v(" to describe data transformations steps:")]),t._v(" "),e("blockquote",[e("p",[t._v("datapackage-pipelines.yaml")])]),t._v(" "),e("div",{staticClass:"language-yaml extra-class"},[e("pre",{pre:!0,attrs:{class:"language-yaml"}},[e("code",[e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("refit-cleaned")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("pipeline")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("run")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" add_metadata\n      "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("parameters")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" refit"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("electrical"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("load"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("measurements\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("title")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'REFIT: Electrical Load Measurements'")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("license")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" CC"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("BY"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("description")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Collection of this dataset was supported by the Engineering and Physical Sciences Research Council (EPSRC) via the project entitled Personalised Retrofit Decision Support Tools for UK Homes using Smart Home Technology (REFIT)"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" which is a collaboration among the Universities of Strathclyde"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Loughborough and East Anglia. The dataset includes data from 20 households from the Loughborough area over the period 2013 "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" 2015. Additional information about REFIT is available from www.refitsmarthomes.org.\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("sources")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("title")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'REFIT: Electrical Load Measurements (Cleaned)'")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("web")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'https://pure.strath.ac.uk/portal/en/datasets/refit-electrical-load-measurements-cleaned(9ab14b0e-19ac-4279-938f-27f643078cec).html'")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("email")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" researchdataproject@strath.ac.uk\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("run")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" add_resource\n      "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("parameters")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'house-1'")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("url")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'datasets/refit-cleaned/House_1.csv'")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("format")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" csv\n\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Other resources are omitted")]),t._v("\n\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("run")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" stream_remote_resources\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("run")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" set_types\n      "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("parameters")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("resources")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"house-[0-9]{1,2}"')]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("types")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v('"Time"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" datetime\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("format")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"fmt:%Y-%m-%d %H:%M:%S"')]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("Unix")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" integer\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("Aggregate")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" integer\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v('"Appliance[1-9]"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" integer\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("run")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" processors.modify_descriptions\n      "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("parameters")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("resources")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" house"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("descriptions")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("Appliance1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("description")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Fridge\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("Appliance2")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("description")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Chest Freezer\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("Appliance3")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("description")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Upright Freezer\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("Appliance4")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("description")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Tumble Dryer\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("Appliance5")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("descripion")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Washing Machine\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("Appliance6")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("description")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Dishwasher\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("Appliance7")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("description")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Computer Site\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("Appliance8")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("description")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Television Site\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("Appliance9")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("description")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Electric Heater\n\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Other resources are omitted")]),t._v("\n\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("run")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dump.to_path\n      "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("parameters")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("out-path")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" packages/refit"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("cleaned\n")])])]),e("p",[t._v("The process follows contains these steps:")]),t._v(" "),e("ul",[e("li",[t._v("Create the data package metadata")]),t._v(" "),e("li",[t._v("Add all data files from the disc")]),t._v(" "),e("li",[t._v("Start resources streaming into the data package")]),t._v(" "),e("li",[t._v("Update resources descriptions using a custom processor")]),t._v(" "),e("li",[t._v("Save the data package to the disc")])]),t._v(" "),e("p",[t._v("Now we’re ready to run this pipeline:")]),t._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[t._v("$ dpp run ./refit-cleaned\n")])])]),e("p",[t._v("After this step we have a data package containing a descriptor:")]),t._v(" "),e("blockquote",[e("p",[t._v("packages/refit-cleaned/datapakcage.json")])]),t._v(" "),e("div",{staticClass:"language-json extra-class"},[e("pre",{pre:!0,attrs:{class:"language-json"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"bytes"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1121187")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"count_of_rows"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("19980")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"description"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Collection of this dataset was supported by the Engineering and Physical Sciences Research Council (EPSRC) via the project entitled Personalised Retrofit Decision Support Tools for UK Homes using Smart Home Technology (REFIT), which is a collaboration among the Universities of Strathclyde, Loughborough and East Anglia. The dataset includes data from 20 households from the Loughborough area over the period 2013 - 2015. Additional information about REFIT is available from www.refitsmarthomes.org."')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"hash"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"433ff35135e0a43af6f00f04cb8e666d"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"license"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"CC-BY-4.0"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"refit-electrical-load-measurements"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"resources"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"bytes"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("55251")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"count_of_rows"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("999")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"dialect"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"delimiter"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('","')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"doubleQuote"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"lineTerminator"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\\r\\n"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"quoteChar"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\\""')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"skipInitialSpace"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("false")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"dpp:streamedFrom"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"datasets/refit-cleaned/House_1.csv"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"dpp:streaming"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"encoding"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"utf-8"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"format"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"csv"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"hash"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ad42fbf1302cabe30e217ff105d5a7fd"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"house-1"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"path"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"data/house-1.csv"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"schema"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"fields"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"format"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"%Y-%m-%d %H:%M:%S"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Time"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"datetime"')]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Unix"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"integer"')]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Aggregate"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"integer"')]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"description"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Fridge"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Appliance1"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"integer"')]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"description"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Chest Freezer"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Appliance2"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"integer"')]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"description"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Upright Freezer"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Appliance3"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"integer"')]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"description"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Tumble Dryer"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Appliance4"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"integer"')]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"descripion"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Washing Machine"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Appliance5"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"integer"')]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"description"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Dishwasher"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Appliance6"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"integer"')]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"description"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Computer Site"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Appliance7"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"integer"')]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"description"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Television Site"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Appliance8"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"integer"')]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"description"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Electric Heater"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Appliance9"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"integer"')]),t._v("\n          "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n      "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n\n    # Other resources is omitted\n\n  "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),e("p",[t._v("And a list of data files linked in the descriptor:")]),t._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[t._v("$ "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("ls")]),t._v(" packages/refit-cleaned/data\nhouse-10.csv  house-13.csv  house-17.csv  house-1.csv   house-2.csv  house-5.csv  house-8.csv\nhouse-11.csv  house-15.csv  house-18.csv  house-20.csv  house-3.csv  house-6.csv  house-9.csv\nhouse-12.csv  house-16.csv  house-19.csv  house-21.csv  house-4.csv  house-7.csv\n")])])]),e("h3",{attrs:{id:"validating-a-data-package-using-goodtables"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#validating-a-data-package-using-goodtables"}},[t._v("#")]),t._v(" Validating a data package using Goodtables")]),t._v(" "),e("p",[t._v("Goodtables is a software family for tabular data validation. It’s available as a Python library, a command line tool, "),e("a",{attrs:{href:"https://try.goodtables.io/",target:"_blank",rel:"noopener noreferrer"}},[t._v("web application"),e("OutboundLink")],1),t._v(" and "),e("a",{attrs:{href:"https://goodtables.io/",target:"_blank",rel:"noopener noreferrer"}},[t._v("continuous validation service"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[t._v("The main features of Goodtables are:")]),t._v(" "),e("ul",[e("li",[t._v("Structural checks: Ensure that there are no empty rows, no blank headers, etc.")]),t._v(" "),e("li",[t._v("Content checks: Ensure that the values have the correct types (“string”, “number”, “date”, etc.), that their format is valid (“string must be an e-mail”), and that they respect the constraints (“age must be a number greater than 18”).")]),t._v(" "),e("li",[t._v("Support for multiple tabular formats: CSV, Excel files, LibreOffice, Data Package, etc.")]),t._v(" "),e("li",[t._v("Parallelized validations for multi-table datasets")])]),t._v(" "),e("p",[t._v("Because we have provided data types for the columns at the wrapping stage, here we validate both the data structure and compliance to the data types using the Goodtables command line interface:")]),t._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[t._v("$ goodtables packages/refit-cleaned/datapackage.json\nDATASET\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'error-count'")]),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(",\n "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'preset'")]),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'datapackage'")]),t._v(",\n "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'table-count'")]),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v(",\n "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'time'")]),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.694")]),t._v(",\n "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'valid'")]),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" True"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),e("h3",{attrs:{id:"modifying-a-data-package-using-packagist"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#modifying-a-data-package-using-packagist"}},[t._v("#")]),t._v(" Modifying a data package using Packagist")]),t._v(" "),e("p",[t._v("If we need to modify our data package, we could use the "),e("a",{attrs:{href:"https://create.frictionlessdata.io/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Packagist"),e("OutboundLink")],1),t._v(". It incorporates a straightforward UI to modify and validate data package descriptors. With its easy to use interface we are able to:")]),t._v(" "),e("ul",[e("li",[t._v("Load/validate/save a data package")]),t._v(" "),e("li",[t._v("Update a data package metadata")]),t._v(" "),e("li",[t._v("Add/remove/modify data package resources")]),t._v(" "),e("li",[t._v("Add/remove/modify data resource fields")]),t._v(" "),e("li",[t._v("Set type/format for data values")])]),t._v(" "),e("p",[e("img",{attrs:{src:s(441),alt:"ADBio"}})]),t._v(" "),e("p",[t._v("On the figure above we have loaded the "),e("code",[t._v("refit-cleaned")]),t._v(" data package into the Packagist UI to make changes to the data package as needed.")]),t._v(" "),e("h3",{attrs:{id:"publishing-a-data-package-to-amazon-s3"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#publishing-a-data-package-to-amazon-s3"}},[t._v("#")]),t._v(" Publishing a data package to Amazon S3")]),t._v(" "),e("blockquote",[e("p",[t._v("Link to the published package: "),e("a",{attrs:{href:"https://s3.eu-central-1.amazonaws.com/pilot-dm4t/pilot-dm4t/packages/refit-cleaned/datapackage.json",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://s3.eu-central-1.amazonaws.com/pilot-dm4t/pilot-dm4t/packages/refit-cleaned/datapackage.json"),e("OutboundLink")],1)])]),t._v(" "),e("p",[t._v("In this section we will show how  data packages can be moved from one data storage system to another. This is possible because it has been containerised.")]),t._v(" "),e("p",[t._v("One important feature of the "),e("code",[t._v("datapackage-pipelines")]),t._v(" project that it works as a conveyor. We could push our data package not only to the local disc but to other destinations. For example to the Amazon S3:")]),t._v(" "),e("blockquote",[e("p",[t._v("pipelines-spec.yml")])]),t._v(" "),e("div",{staticClass:"language-yaml extra-class"},[e("pre",{pre:!0,attrs:{class:"language-yaml"}},[e("code",[e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("refit-cleaned")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Initial steps are omitted")]),t._v("\n\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("run")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" aws.dump.to_s3\n      "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("parameters")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("bucket")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" pilot"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("dm4t\n        "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("path")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" pilot"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("dm4t/packages/refit"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("cleaned\n")])])]),e("p",[t._v("Running this command again:")]),t._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[t._v("$ dpp run ./refit-cleaned\n")])])]),e("p",[t._v("And now our data package is published to Amazon the S3 remote storage:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://i.imgur.com/5Z7EPDR.pnghttps://",alt:"screenshot of S3 storage"}})]),t._v(" "),e("h3",{attrs:{id:"getting-insight-from-data-using-python-libraries"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#getting-insight-from-data-using-python-libraries"}},[t._v("#")]),t._v(" Getting insight from data using Python libraries")]),t._v(" "),e("blockquote",[e("p",[t._v("Link to the demostration script: "),e("a",{attrs:{href:"https://github.com/frictionlessdata/pilot-dm4t/blob/delivery/scripts/refit-cleaned.py",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/frictionlessdata/pilot-dm4t/blob/delivery/scripts/refit-cleaned.py"),e("OutboundLink")],1)])]),t._v(" "),e("p",[t._v("The Frictionless Data projects provides various Python (along with other 8 languages) libraries to work with data package programatically. We used the "),e("code",[t._v("datapackage")]),t._v(" library to analyse the "),e("code",[t._v("refit-cleaned")]),t._v(" data package:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" datetime\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" statistics\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" datapackage "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Package\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Get aggregates")]),t._v("\nconsumption "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\npackage "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Package"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'packages/refit-cleaned/datapackage.json'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" resource "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" package"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resources"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" row "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" resource"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("iter")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("keyed"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        hour "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" row"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Time'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hour\n        consumption"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setdefault"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hour"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        consumption"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("hour"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Aggregate'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Get averages")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" hour "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" consumption"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    consumption"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("hour"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" statistics"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("consumption"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("hour"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Print results")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" hour "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sorted")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("consumption"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Average consumption at %02d hours: %.0f'")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hour"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" consumption"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("hour"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Now we could run it in the command line:")]),t._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[t._v("$ python examles/refit-cleaned.py\nAverage consumption at 00 hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("232")]),t._v("\nAverage consumption at 01 hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("213")]),t._v("\nAverage consumption at 02 hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("247")]),t._v("\nAverage consumption at 03 hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("335")]),t._v("\nAverage consumption at 04 hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("215")]),t._v("\nAverage consumption at 05 hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("690")]),t._v("\nAverage consumption at 06 hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("722")]),t._v("\nAverage consumption at 07 hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("648")]),t._v("\nAverage consumption at 08 hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("506")]),t._v("\nAverage consumption at 09 hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("464")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("364")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("11")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("569")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("520")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("13")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("497")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("14")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("380")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("383")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("459")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("17")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("945")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("18")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("733")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("19")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("732")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("471")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("21")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("478")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("22")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("325")]),t._v("\nAverage consumption at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("23")]),t._v(" hours: "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("231")]),t._v("\n")])])]),e("p",[t._v("Here we we’re able to get the averages for electricity consumption grouped by hour. We could have achieved this in different ways, but using the Frictionless Data specs and software provides some important advantages:")]),t._v(" "),e("ul",[e("li",[t._v("The fact that we have data wrapped into a data package has allowed us to validate and read the data already converted for its correct types (e.g native python "),e("code",[t._v("datetime")]),t._v(" object). No need for any kind of string parsing.")]),t._v(" "),e("li",[t._v("The Frictionless Data software uses file streams under the hood. This means that only the current row is kept in memory, so we’re able to handle datasets bigger than the available RAM memory.")])]),t._v(" "),e("h3",{attrs:{id:"exporting-data-to-an-elasticsearch-cluster"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#exporting-data-to-an-elasticsearch-cluster"}},[t._v("#")]),t._v(" Exporting data to an ElasticSearch cluster")]),t._v(" "),e("blockquote",[e("p",[t._v("Link to the export script: "),e("a",{attrs:{href:"https://github.com/frictionlessdata/pilot-dm4t/blob/delivery/scripts/refit-cleaned.py",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/frictionlessdata/pilot-dm4t/blob/delivery/scripts/refit-cleaned.py"),e("OutboundLink")],1)])]),t._v(" "),e("p",[t._v("The Frictionless Data software provides plugins to export data to various backends like SQL, BigQuery etc. We will export the first resource from our data package for future analysis:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" elasticsearch "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Elasticsearch\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" datapackage "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Package\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tableschema_elasticsearch "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Storage\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Get resource")]),t._v("\npackage "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Package"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'packages/refit-cleaned/datapackage.json'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nresource "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" package"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_resource"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'house-1'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Create storage")]),t._v("\nengine "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Elasticsearch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstorage "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Storage"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("engine"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Write data")]),t._v("\nstorage"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'refit-cleaned'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'house-1'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" resource"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("schema"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("descriptor"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("storage"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'refit-cleaned'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'house-1'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" resource"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("keyed"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Unix'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),e("p",[t._v("Now we are able to check that our documents are indexed:")]),t._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[t._v("$ http http://localhost:9200/_cat/indices?v\n")])])]),e("h3",{attrs:{id:"getting-insight-from-data-using-kibana"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#getting-insight-from-data-using-kibana"}},[t._v("#")]),t._v(" Getting insight from data using Kibana")]),t._v(" "),e("p",[t._v("To demonstrate how the Frictionless Data specs and software empower the usage of other analytics tools, we will use ElasticSearch/Kibana project. On the previous step we have imported our data package into an ElasticSearch cluster. It allows us to visualize data using a simple UI:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://i.imgur.com/Fm373F4.png",alt:"screenshot of elasticsearch cluster"}})]),t._v(" "),e("p",[t._v("In this screenshot we see the distribution of the average electricity comsumption. This is just an example of what you can do by having the ability to easily load datasets into other analytical software.")]),t._v(" "),e("h2",{attrs:{id:"review"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#review"}},[t._v("#")]),t._v(" Review")]),t._v(" "),e("h3",{attrs:{id:"the-results"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#the-results"}},[t._v("#")]),t._v(" The results")]),t._v(" "),e("p",[t._v("In this pilot, we have been able to demonstrate the the following:")]),t._v(" "),e("ul",[e("li",[t._v("Packaging the "),e("code",[t._v("refit-cleaned")]),t._v(" dataset as a data package using the Data Package Pipelines library")]),t._v(" "),e("li",[t._v("Validating the data package using the Goodtables library")]),t._v(" "),e("li",[t._v("Modifying data packages metadata using the Packagist UI")]),t._v(" "),e("li",[t._v("Uploading the dataset to Amazon S3 and ElasticSearch cluster using Frictionless Data tools")]),t._v(" "),e("li",[t._v("Reading and analysing in Python the created Data Package using the Frictionless Data library")])]),t._v(" "),e("h3",{attrs:{id:"current-limitations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#current-limitations"}},[t._v("#")]),t._v(" Current limitations")]),t._v(" "),e("p",[t._v("The central challenge of working with these datasets is the size. Publishing the results of these research projects as flat files for immediate analysis is beneficial, however, the scale of each of these datasets (gigabytes of data, millions of rows) is a challenge to deal with no matter how you are storing. Processing this data through Data Package pipelines takes a long time.")]),t._v(" "),e("h3",{attrs:{id:"next-steps"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#next-steps"}},[t._v("#")]),t._v(" Next Steps")]),t._v(" "),e("ul",[e("li",[t._v("Improve the speed of the data package creation step")])]),t._v(" "),e("h3",{attrs:{id:"find-out-more"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#find-out-more"}},[t._v("#")]),t._v(" Find Out More")]),t._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://github.com/frictionlessdata/pilot-pnnl",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/frictionlessdata/pilot-pnnl"),e("OutboundLink")],1)])]),t._v(" "),e("h3",{attrs:{id:"source-material"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#source-material"}},[t._v("#")]),t._v(" Source Material")]),t._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://app.hubspot.com/sales/2281421/deal/146418008",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://app.hubspot.com/sales/2281421/deal/146418008"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://discuss.okfn.org/c/working-groups/open-archaeology",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://discuss.okfn.org/c/working-groups/open-archaeology"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://github.com/frictionlessdata/pilot-open-archaeology",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/frictionlessdata/pilot-open-archaeology"),e("OutboundLink")],1)])])])}),[],!1,null,null,null);a.default=n.exports}}]);